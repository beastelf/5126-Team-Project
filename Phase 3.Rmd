---
title: "Supervised Model"
author: "Ruiqing Huo (300478570)"
date: "2025-11-07"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(gains)
library(xgboost)
library(randomForest)
```

## 1. Inport the dataset
```{r, echo=TRUE}
red <- read_csv("~/Desktop/winequality-red-renamed.csv")
white <- read_csv("~/Desktop/winequality-white-renamed.csv")

table(red$quality)
table(white$quality)
```
## 2. Combine two datasets
We add the columns for distinguishing two different types of wine. We use 0 and 1 to define whether the wine belongs to the type, like in red wine dataset, type_red should be 1. Then, combining them into one dataset.
```{r, echo=TRUE}
white['type_white']=1
white['type_red']=0
red['type_white']=0
red['type_red']=1
wine_all <- bind_rows(red, white)
```


## 3. Split dataset 
In this step, we split dataset into testing (40%) and training (60%). Also, since there are spaces in the column names in these dataset, we rename them for using in the following steps.
```{r, echo=TRUE}
create_train_test <- function(wine_all, size = 0.6, train = TRUE) {
  n_row = nrow(red)
  total_row = size * n_row
  train_sample <- 1:total_row
  if (train == TRUE) {
    return (red[train_sample, ])
  } else {
    return (red[-train_sample, ])
  }
}
all_train <- create_train_test(wine_all, 0.6, train = TRUE)
all_test <- create_train_test(wine_all, 0.6, train = FALSE)

names(all_train) <- make.names(names(all_train))
names(all_test)  <- make.names(names(all_test))
```


## 4. Group the dataset
We decided to group the dataset as the following way, if the quality is lower than 6, then it belongs to "low" quality, and the others are "high" quality.
```{r, echo=TRUE}
all_train$label <- factor(ifelse(all_train$quality <= 6, "low", "high"))
all_test$label  <- factor(ifelse(all_test$quality <= 6, "low", "high"))
```


## 5. Decision Tree
```{r, echo=TRUE}
all_train$quality <- NULL
all_test$quality  <- NULL

fit <- rpart(label ~ ., data = all_train, method = "class",
             control = rpart.control(minbucket = 50, maxdepth = 7, cp = 0.01))
rpart.plot(fit, extra = 106) 

pred <- predict(fit, newdata = all_test, type = "class")

table_mat <- table(all_test$label, pred)
table_mat
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
print(paste('Accuracy for Decision Tree:', accuracy_Test))


pred_prob <- predict(fit, newdata = all_test, type = "prob")[,2]
g <- gains(actual = as.numeric(all_test$label) - 1,
           predicted = pred_prob, groups = 10)
plot(g,
     main = "Lift Chart for Decision Tree",
     xlab = "Depth of File (Deciles)",
     ylab = "Mean Response",
     col = c("red", "green", "skyblue"),
     lty = c(1,1,1),
     legend = c("Mean Response", "Cumulative Mean Response", "Mean Predicted Response"))
```
Rules:
(1). If the alcohol >= 12, then it's high quality.
(2). If the alcohol < 12, then it's low quality.

### Confusion matrix and F1 score
```{r, echo=TRUE}
library(caret)
pred <- predict(fit, newdata = all_test, type = "class")
confusionMatrix(all_test$label, pred)
precision <- 0.5732
recall <- 0.3672
F1 <- 2 * (precision * recall) / (precision + recall)
print(paste('F1 score:', F1))
```

The accuracy of this decision tree model is 0.7867, indicating that it correctly classified about 78.67% of the these wine samples as high or low quality. By looking at the above lift chart, we can see that this lift chart is in decreasing trend, which means that the model is capable of ranking effectively by predicting the quality probability. However, we can see that F1 score is around 0.4476. Although the accuracy is high but F1 score is low, which indicates that the model cannot balance the precision and recall, especially in high quality wine.    

## 6. XGBoost
```{r, echo=TRUE}
set.seed(42)

train_x <- model.matrix(label ~ ., all_train)[, -1]
test_x  <- model.matrix(label ~ ., all_test)[, -1]

train_y <- ifelse(all_train$label == "high", 1, 0)
test_y  <- ifelse(all_test$label == "high", 1, 0)

dtrain <- xgb.DMatrix(data = train_x, label = train_y)
dtest  <- xgb.DMatrix(data = test_x, label = test_y)

params <- list(
  objective = "binary:logistic",  
  max_depth = 4,
  eta = 0.1,
  subsample = 0.8,
  colsample_bytree = 0.8
)

xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  watchlist = list(train = dtrain, test = dtest),
  print_every_n = 10,
  early_stopping_rounds = 10
)

pred_prob <- predict(xgb_model, newdata = test_x)
pred_label <- ifelse(pred_prob > 0.5, 1, 0)
acc <- mean(pred_label == test_y)
cat("Accuracy for XGBoost:", acc)
```
From the above result, we can see that the accuracy is around 0.8787, which means that this model correctly classified about 87.87% of the these wine samples as high or low quality. This result shows the XGBoost model has a strong overall predictive performance.

## 7. Random Forest
```{r, echo=TRUE}
wine_RF <- randomForest(label~., data = all_train)

predict_RF <- predict(wine_RF, all_test)
table_RF <- table(all_test$label, predict_RF)
table_RF
print(paste('Accuracy for randon forest test', sum(diag(table_RF)) / sum(table_RF)))
```
From above result, we can see that the accuracy for random forest is 0.8695, which shows that this model correctly classified about 86.95% of the these wine samples as high or low quality.


## Conclusion:
By comparing the above three models, we can see that XGBoost and Random Forest have higher accuracy than Decision Tree model. The best of these three model is XGBoost, which means this model has the strongest reliable predictive accuracy and we choose this model for this project. By comparing the result of separating dataset, we found that the accuracy in combining them is much higher, which means combining the dataset is better choice ofr analyzing.
